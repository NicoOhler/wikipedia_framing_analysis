{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T23:30:09.340711900Z",
     "start_time": "2024-03-06T23:30:09.297295200Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import wikipediaapi\n",
    "import requests\n",
    "import nltk\n",
    "# nltk.download('punkt') # needed to download punkt once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T23:30:18.472547800Z",
     "start_time": "2024-03-06T23:30:18.465924600Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_string(\n",
    "    text, remove_headers=True, remove_enumerations=True\n",
    "):  # ? might not work for all articles, need to test\n",
    "    text = re.split(r\"==See also==\", text)[0]  # remove everything after \"See also\"\n",
    "    text = re.sub(\n",
    "        r\"\\{\\{Asof\\|(\\d{4})\\|(\\d{1,2})\\}\\}\", r\"as of \\2/\\1\", text\n",
    "    )  # replace {{Asof|YYYY|MM}} with as of MM/YYYY\n",
    "    text = re.sub(r\"\\{\\{.*}}\", \"\", text)  # remove {{}}\n",
    "    text = re.sub(r\"&lt;!--.*?-->\", \"\", text)  # remove <!--...-->\n",
    "    text = re.sub(r\"&lt.*--\", \"\", text)  # remove &lt;!--\n",
    "    text = re.sub(r\"-->\", \"\", text)  # remove -->\n",
    "    if remove_enumerations:\n",
    "        text = re.sub(r\"\\n\\*.*\", \"\\n\", text)\n",
    "    else:\n",
    "        text = re.sub(r\"\\n\\* \", \"\\n\", text)  # remove enumeration symbol *\n",
    "    text = re.sub(r\"\\n#\", \"\\n\", text)  # remove enumeration symbol #\n",
    "    text = re.sub(r\"&lt;ref.*?&lt;/ref>\", \"\", text)  # remove <ref>...</ref>\n",
    "    text = re.sub(r\"&lt;ref.*/>\", \"\", text)  # remove <ref ... />\n",
    "    if remove_headers:\n",
    "        text = re.sub(\n",
    "            r\"==+.*==+\\n\", \"\", text\n",
    "        )  # remove lines containing ==, ===, ====, ...\n",
    "    else:\n",
    "        text = re.sub(r\"==+\", \"\", text)  # remove ==, ===, ====, ...\n",
    "    text = re.sub(r\"'+\", \"\", text)  # remove ''', '''', ...\n",
    "    text = re.sub(r\"\\xa0\", \" \", text)  # replace non-breaking space with space\n",
    "    text = re.sub(\n",
    "        r\"\\[\\[File:(?:\\[\\[[^\\]]*?\\]\\]|.)*?\\]\\]\", \"\", text\n",
    "    )  # remove [[File:...]]\n",
    "    text = re.sub(\n",
    "        r\"\\[\\[[^\\|\\]]*\\|([^\\]]+)\\]\\]\", r\"[[\\1]]\", text\n",
    "    )  # replace [[left|right]] with [right]\n",
    "    text = text.replace(r\"[[\", \"\").replace(\"]]\", \"\")  # remove [[ and ]]\n",
    "    text = re.sub(\n",
    "        r\"\\{\\|(?:(?:\\{\\|(?:(?:\\{\\|(?:[^{}])*\\|\\})|(?:[^{}]))*\\|\\})|(?:[^{}]))*\\|\\}\",\n",
    "        \"\",\n",
    "        text,\n",
    "    )  # replace nested {| * |}\n",
    "    text = re.sub(r\"\\{\\{(?:\\n|.)*?\\}\\}\", \"\", text)  # replace {{ * }}\n",
    "    text = re.sub(r\"mini\\|.*\\|\", \"\", text)\n",
    "    text = re.sub(r\"mini\\|\", \"\", text)\n",
    "    text = re.sub(r\":\\* .*ISBN.*\", \"\", text)  # remove reverence books\n",
    "    text = re.sub(r\"Kategorie:.*\", \"\", text)  # remove reverence books\n",
    "    text = re.sub(r\"\\n+\", \"\\n\", text)  # replace multiple newlines with one\n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_source(text):\n",
    "    text = re.split(r\"<textarea[^>]*>\", text)[1]\n",
    "    return re.split(r\"</textarea>\", text)[0]\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_articles(articles):\n",
    "    tokenized_articles = []\n",
    "    for article in articles:\n",
    "        tokenized_articles.append(nltk.sent_tokenize(article))\n",
    "    return tokenized_articles\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T23:46:21.398984Z",
     "start_time": "2024-03-06T23:46:21.383425800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fetch_article_api(article_title, language=\"en\"):\n",
    "    wiki = wikipediaapi.Wikipedia(\n",
    "        \"FramingAnalysis (riedl.manuel.privat@gmail.com)\",\n",
    "        language,\n",
    "        extract_format=wikipediaapi.ExtractFormat.WIKI,\n",
    "    )\n",
    "    page = wiki.page(article_title)\n",
    "    article = page.text\n",
    "    if language == \"en\":\n",
    "        article = re.split(r\"\\nSee also\\n\", article)[0]\n",
    "        article = re.split(r\"\\nReferences\\n\", article)[0]\n",
    "        article = re.split(r\"\\nSignificant publications\\n\", article)[0]\n",
    "        article = re.split(r\"\\nPublications\\n\", article)[0]\n",
    "        article = re.split(r\"\\nPublications\\n\", article)[0]\n",
    "        article = re.split(r\"\\n== References ==\", article)[0]\n",
    "\n",
    "    return article\n",
    "\n",
    "\n",
    "def fetch_wiki_articles_http(\n",
    "    article_title, language=\"en\"\n",
    "): \n",
    "    response = requests.get(\n",
    "        f\"https://{language}.wikipedia.org/w/index.php?title={article_title}&action=edit\"\n",
    "    )\n",
    "    text = extract_source(response.text)\n",
    "    return clean_string(text)\n",
    "\n",
    "def fetch_article_locally(article_title):\n",
    "    with open(\"articles/\" + article_title + \".txt\", \"r\", encoding=\"utf8\") as file:\n",
    "        return file.read()\n",
    "\n",
    "def fetch_articles(article_titles, language=\"en\", fetch_method=\"api\"):\n",
    "    articles = []\n",
    "    for article_title in article_titles:\n",
    "        if fetch_method == \"api\":\n",
    "            article = fetch_article_api(article_title, language)\n",
    "        elif fetch_method == \"http\":\n",
    "            article = fetch_wiki_articles_http(article_title, language)\n",
    "        elif fetch_method == \"local\":\n",
    "            article = fetch_article_locally(article_title)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid fetch method\")\n",
    "        articles.append(article)\n",
    "   \n",
    "    return articles\n",
    "\n",
    "\n",
    "def save_articles_locally(articles, file_names, path=\"articles/\"):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    for i in range(len(articles)):\n",
    "        with open(path + file_names[i] + \".txt\", \"w\", encoding=\"utf8\") as file:\n",
    "            file.write(articles[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./article_titles_by_category/\"\n",
    "article_titles = []\n",
    "for file in os.listdir(path):\n",
    "    with open(path + file, \"r\") as f:\n",
    "        article_titles.extend(f.read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#articles_http = fetch_articles(article_titles, fetch_method=\"http\")\n",
    "#save_articles_locally(articles_http, article_titles, path=\"articles_http/\")\n",
    "\n",
    "articles_api = fetch_articles(article_titles, fetch_method=\"api\")\n",
    "save_articles_locally(articles_api, article_titles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
